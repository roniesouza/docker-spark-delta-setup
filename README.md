# üöÄ Spark + Delta Lake com VS Code Remote

Este setup fornece um **container Docker enxuto** para trabalhar com **Apache Spark 3.5.3** e **Delta Lake**, permitindo desenvolvimento remoto com **VS Code Remote - Containers**.  
√â ideal para estudos e tamb√©m pode servir como base para ambientes Spark mais complexos, sem a necessidade de configurar Spark, Java ou Python manualmente.

O volume mapeado garante que notebooks e dados persistam ap√≥s reiniciar o container, e configura√ß√µes adicionais do Spark (como mem√≥ria ou n√∫mero de cores) podem ser ajustadas via vari√°veis de ambiente no `docker run`.

A imagem final possui aproximadamente **2.7 GB**, o que √© considerado normal para ambientes Spark, j√° que inclui Spark, Hadoop, Java, Scala e depend√™ncias adicionais do Python. Esse tamanho est√° dentro da m√©dia (2‚Äì4 GB) encontrada em setups semelhantes. Para quem busca algo mais leve, seria necess√°rio montar uma imagem baseada em vers√µes ‚Äúslim‚Äù do Python ou do OpenJDK e instalar o Spark manualmente, mas isso aumenta a complexidade do setup.

---

## üì¶ Configura√ß√£o do container

* **Base:** `bitnami/spark:3.5.3` (Debian 12, Spark 3.5.3, OpenJDK 17)
* **Python:** 3.12.8
* **PySpark:** 3.5.3
* **Scala:** 2.12.18
* **Delta Lake:** delta-spark 3.3.2
* **Diret√≥rio de trabalho:** `/home`
* **Depend√™ncias adicionais:** `ipykernel` (para notebooks)

---

## üê≥ Dockerfile

```dockerfile
FROM bitnami/spark:3.5.3

WORKDIR /home

USER root

RUN apt-get update && \
    apt-get install -y --no-install-recommends git && \
    pip install --no-cache-dir delta-spark==3.3.2 ipykernel && \
    rm -rf /var/lib/apt/lists/*
```

**O que ele faz agora, detalhado:**

1. **Imagem base:** Utiliza `bitnami/spark:3.5.3`, que j√° inclui Spark, Java, Hadoop e Scala.
2. **Diret√≥rio de trabalho:** Define `/home` como diret√≥rio de trabalho dentro do container.
3. **Usu√°rio root:** Permite instalar pacotes do sistema.
4. **Instala√ß√µes adicionais:**

   * `git`: para versionamento e clonagem de reposit√≥rios.
   * `delta-spark==3.3.2`: para integra√ß√£o com Delta Lake no Spark.
   * `ipykernel`: para executar notebooks Python dentro do container.
5. **Limpeza:** Remove cache do `apt` para reduzir o tamanho final da imagem.

---

## ‚öôÔ∏è Passos para rodar o container

1. Crie o diret√≥rio local que ser√° usado como volume:

```bash
mkdir spark-delta-project
cd spark-delta-project
```



2. Construa a imagem Docker:

```bash
docker build -t spark-delta-jupyter .
```

3. Rode o container:

```bash
docker run -d \
  -u 0 \
  -v $(pwd):/home \
  --name spark-delta-container \
  spark-delta-jupyter
```

**Explica√ß√£o dos par√¢metros:**

* `-d` ‚Üí roda o container em **modo detach** (em segundo plano).
* `-u 0` ‚Üí executa como **root** dentro do container (evita problemas de permiss√£o).
* `-v $(pwd):/home` ‚Üí mapeia o diret√≥rio local para `/home` do container.
* `--name spark-delta-container` ‚Üí nome do container, facilitando opera√ß√µes futuras.
* `spark-delta-jupyter` ‚Üí nome da imagem constru√≠da.

---

## üß™ Testando Delta Lake

Crie um script Python ou notebook:

```python
from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_pip

spark = configure_spark_with_delta_pip(
    SparkSession.builder
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
).getOrCreate()

df = spark.createDataFrame([("Alice", 34), ("Bob", 45)], ["name", "age"])
df.write.format("delta").mode("overwrite").save("/tmp/delta-table")
spark.read.format("delta").load("/tmp/delta-table").show()
```

> ‚úÖ Isso verifica se o **Delta Lake** est√° funcionando corretamente no container.

---

## üñ• VS Code Remote Connection

O **Remote - Containers** permite trabalhar **diretamente dentro do container**:

### Benef√≠cios

* Evita instalar Spark, Java ou Python localmente.
* Ambiente isolado e reprodut√≠vel.
* Acesso direto a notebooks e scripts com Spark + Delta Lake prontos.
* Facilita setups complexos sem configura√ß√£o manual.

### Como usar

1. Instale a extens√£o [Remote - Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers).
2. Abra o VS Code no diret√≥rio local do projeto.
3. Pressione `F1` ‚Üí **Remote-Containers: Attach to Running Container‚Ä¶**
4. Escolha `spark-delta-container`.
5. O VS Code estar√° conectado ao container; execute notebooks, scripts ou terminais Python com Spark e Delta prontos.

## ‚ö†Ô∏è Compatibilidade de vers√µes

Um detalhe muitas vezes negligenciado, mas que pode causar **erros dif√≠ceis de diagnosticar**, √© a compatibilidade entre as ferramentas.
No ecossistema Spark + Delta Lake, as vers√µes precisam estar alinhadas para que tudo funcione corretamente.

No exemplo deste setup, usamos:

* **Apache Spark:** 3.5.3
* **Delta Lake (delta-spark):** 3.3.2

Essa escolha n√£o √© aleat√≥ria: a compatibilidade entre **Spark 3.5.x** e **Delta Lake 3.3.x** √© documentada oficialmente.
Negligenciar esse cuidado pode gerar problemas como falhas ao inicializar sess√µes Spark, erros de schema ou at√© corromper tabelas Delta.

### üîó Compatibilidade oficial (Delta Lake x Spark)

| Delta Lake version | Apache Spark version  |
| ------------------ | --------------------- |
| 4.0.x              | 4.0.x                 |
| 3.3.x              | 3.5.x                 |
| 3.2.x              | 3.5.x                 |
| 3.1.x              | 3.5.x                 |
| 3.0.x              | 3.5.x                 |
| 2.4.x              | 3.4.x                 |
| 2.3.x              | 3.3.x                 |
| 2.2.x              | 3.3.x                 |
| 2.1.x              | 3.3.x                 |
| 2.0.x              | 3.2.x                 |
| 1.2.x              | 3.2.x                 |
| 1.1.x              | 3.2.x                 |
| 1.0.x              | 3.1.x                 |
| 0.7.x / 0.8.x      | 3.0.x                 |
| < 0.7.0            | 2.4.2 ‚Äì 2.4.\<latest> |

üìñ Fonte: [Documenta√ß√£o oficial do Delta Lake](https://docs.delta.io/releases/#compatibility-with-apache-spark)

---

üëâ **Case pr√°tico:**
Quando montei o setup inicial, ignorei esse detalhe de compatibilidade e perdi horas tentando corrigir erros que n√£o faziam sentido.
S√≥ depois de voltar √† documenta√ß√£o oficial percebi que estava usando uma vers√£o do Delta incompat√≠vel com o Spark ‚Äî o que tornava o ambiente inst√°vel.

Esse epis√≥dio refor√ßou ainda mais o aprendizado: **n√£o existe atalho fora da documenta√ß√£o oficial**. Conferir tabelas de compatibilidade como essa deve ser sempre o primeiro passo antes de configurar qualquer ambiente.

---

## üí° Aprendizado

O grande aprendizado que fica √© simples, mas essencial: **sempre focar na documenta√ß√£o oficial**.

Negligenciar esse passo √© uma falha grave ‚Äî foi exatamente isso que aconteceu comigo.
Os melhores sempre repetem essa li√ß√£o, mas eu ignorei e acabei pagando o pre√ßo.

O caminho mais r√°pido e seguro nunca √© sair tentando milhares de solu√ß√µes de v√≠deos, f√≥runs ou jogando perguntas no GPT sem crit√©rio.
O verdadeiro atalho √© **ler e entender a documenta√ß√£o oficial, direto de quem desenvolveu**.

Quando n√£o seguimos isso, ca√≠mos no imediatismo: testamos mil coisas sem compreender de fato, gastamos tempo demais procurando respostas r√°pidas e, no fim, a solu√ß√£o estava l√° desde o in√≠cio na documenta√ß√£o.
